version: '3.9'

services:
  postgres:
    image: postgres:14
    container_name: postgres
    environment:
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin
      POSTGRES_DB: superset
    volumes:
      - pg_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  spark-master:
    image: apache/spark:3.4.4
    container_name: spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "7077:7077"
      - "8080:8080"
    environment:
      SPARK_MODE: master
    volumes:
      - ./spark-apps:/opt/spark-apps

  spark-worker:
    image: apache/spark:3.4.4
    container_name: spark-worker
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      SPARK_MODE: worker
    volumes:
      - ./spark-apps:/opt/spark-apps

  spark-submit:
    image: apache/spark:3.4.4
    container_name: spark-submit
    depends_on:
      - spark-master
    environment:
      HOME: /opt/spark-apps
    volumes:
      - ./spark-apps:/opt/spark-apps
      - ./ivy:/opt/spark-apps/.ivy2
    command: >
      bash -c "
        pip install clickhouse-driver &&
        pip install pandas &&
        sleep infinity
      "
      

  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9000:9000"  # S3 API
      - "9001:9001"  # Web UI
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: admin123
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data

  clickhouse:
    image: clickhouse/clickhouse-server:latest
    environment:
    - CLICKHOUSE_USER=admin
    - CLICKHOUSE_PASSWORD=admin123
    container_name: clickhouse
    ports:
      - "8123:8123"
      - "9002:9000"
    volumes:
      - clickhouse_data:/var/lib/clickhouse

  superset:
    build:
      context: ./superset
      dockerfile: Dockerfile
    ports:
      - "8088:8088"
    depends_on:
      - postgres
    environment:
      SUPERSET_SECRET_KEY: "admin"
      DATABASE_URL: postgresql+psycopg2://admin:admin@postgres:5432/superset
    command: >
      /bin/sh -c "
      superset db upgrade &&
      superset fab create-admin --username admin --firstname Superset --lastname Admin --email admin@superset.com --password admin &&
      superset init &&
      superset run -h 0.0.0.0 -p 8088
      "

  init:
    build:
      context: ./init
    container_name: init
    hostname: init
    volumes:
      - ./init:/home/init
    depends_on:
      - postgres
      - minio
      - clickhouse
      - superset
    command: >
      bash -c "
        pip install -r requirements.txt &&
        python3 create_bucket.py &&
        sleep infinity
      "

  airflow-redis:
    image: redis:latest
    container_name: airflow-redis
    ports:
      - "6379:6379"

  airflow-webserver:
    image: apache/airflow:2.7.1
    container_name: airflow-webserver
    depends_on:
      - airflow-redis
      - postgres
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://admin:admin@postgres:5432/superset
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://admin:admin@postgres:5432/superset
      AIRFLOW__CELERY__BROKER_URL: redis://airflow-redis:6379/0
      AIRFLOW__CORE__FERNET_KEY: 'qRKvF-3zfLhGwVTIUOBsR2Bogv-4dQ7FdxsV4Bls-Kc='
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    ports:
      - "8082:8080" # Airflow Web UI
    volumes:
      - ./airflow:/opt/airflow
    command: >
      bash -c "
        airflow db migrate &&
        airflow users create --email email --firstname firstname --lastname lastname --password admin --role Admin --username admin &&
        airflow standalone &&
        airflow dags trigger pipeline
      "

  airflow-scheduler:
    image: apache/airflow:2.7.1
    container_name: airflow-scheduler
    depends_on:
      - airflow-webserver
      - airflow-redis
      - postgres
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://admin:admin@postgres:5432/superset
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://admin:admin@postgres:5432/superset
      AIRFLOW__CELERY__BROKER_URL: redis://airflow-redis:6379/0
      AIRFLOW__CORE__FERNET_KEY: 'YOUR_FERNET_KEY_HERE'
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    volumes:
      - ./airflow:/opt/airflow
    command: scheduler

  airflow-worker:
    image: apache/airflow:2.7.1
    container_name: airflow-worker
    depends_on:
      - airflow-webserver
      - airflow-redis
      - postgres
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://admin:admin@postgres:5432/superset
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://admin:admin@postgres:5432/superset
      AIRFLOW__CELERY__BROKER_URL: redis://airflow-redis:6379/0
      AIRFLOW__CORE__FERNET_KEY: 'YOUR_FERNET_KEY_HERE'
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    volumes:
      - ./airflow:/opt/airflow
    command: celery worker

volumes:
  pg_data:
  minio_data:
  clickhouse_data:
  airflow_pgdata: